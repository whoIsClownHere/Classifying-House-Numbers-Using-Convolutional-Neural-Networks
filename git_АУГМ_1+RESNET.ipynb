{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT3yQpbveabJ",
        "outputId": "6b37b64a-8af1-4fd3-dad9-def8e5afd5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading ml-intensive-of-yandex-academy-2023.zip to /content\n",
            " 97% 255M/263M [00:02<00:00, 141MB/s] \n",
            "100% 263M/263M [00:02<00:00, 99.1MB/s]\n",
            "Archive:  ml-intensive-of-yandex-academy-2023.zip\n",
            "  inflating: data_test               \n",
            "  inflating: data_train              \n",
            "  inflating: meta                    \n",
            "Train data keys: dict_keys(['section', 'labels', 'images'])\n",
            "Test data keys: dict_keys(['section', 'images'])\n",
            "\n",
            "Shape of train images: (50000, 32, 32, 3)\n",
            "Shape of test images: (25000, 32, 32, 3)\n",
            "\n",
            "Unique labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
            "\n",
            "Classes distribution: 1    0.19300\n",
            "2    0.15190\n",
            "3    0.11250\n",
            "4    0.09882\n",
            "5    0.09208\n",
            "7    0.07752\n",
            "6    0.07718\n",
            "0    0.06682\n",
            "8    0.06664\n",
            "9    0.06354\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "print(tf.__version__)\n",
        "\n",
        "!mkdir /root/.kaggle/\n",
        "# with open(\"/root/.kaggle/kaggle.json\", \"w\") as file:\n",
        "#     file.write('{\"username\":\"burindima\",\"key\":\"---\"}')\n",
        "!kaggle competitions download -c ml-intensive-of-yandex-academy-2023\n",
        "!unzip ml-intensive-of-yandex-academy-2023.zip \n",
        "\n",
        "train_data = pd.read_pickle('data_train')\n",
        "test_data = pd.read_pickle('data_test')\n",
        "\n",
        "print(f'Train data keys: {train_data.keys()}')\n",
        "print(f'Test data keys: {test_data.keys()}\\n')\n",
        "print(f'Shape of train images: {train_data[\"images\"].shape}')\n",
        "print(f'Shape of test images: {test_data[\"images\"].shape}\\n')\n",
        "print(f'Unique labels: {set(train_data[\"labels\"])}\\n')\n",
        "print(f'Classes distribution: {pd.Series(train_data[\"labels\"]).value_counts(normalize=True)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(50000* 0.15190)\n",
        "print(50000*0.19300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOkQYJaTfD-S",
        "outputId": "34d30507-1606-4960-fb85-7c36b02b23b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7595.0\n",
            "9650.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ЧБ"
      ],
      "metadata": {
        "id": "Enuas8TmfE6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def black_white_point(image):\n",
        "    # получаем координаты черных пикселей\n",
        "  black_pixels = np.where(np.all(image == [0, 0, 0], axis=-1))\n",
        "  # print(black_pixels)\n",
        "\n",
        "  # заменяем черные пиксели на среднее значение пикселей в окрестности 3x3\n",
        "  for i, j in zip(black_pixels[0], black_pixels[1]):\n",
        "      # получаем окрестность 3x3 пикселя\n",
        "      neighborhood = image[max(0, i-1):min(i+2, image.shape[0]), max(0, j-1):min(j+2, image.shape[1])]\n",
        "      # отсеиваем соль и перец чтобы не мешались\n",
        "      neighborhood1 = neighborhood.reshape((-1, 3))\n",
        "      neighborhood1 = neighborhood1[np.nonzero(np.sum(neighborhood1, axis=1))]\n",
        "      neighborhood1 = neighborhood1[np.nonzero(np.sum(neighborhood1, axis=1) != 255*3)]\n",
        "      if not neighborhood1.any():  # на случай если пиксель полностью окружён шумом\n",
        "          neighborhood1 = neighborhood\n",
        "      # считаем среднее значение пикселей\n",
        "      mean_pixel = np.mean(neighborhood, axis=(0, 1))\n",
        "      # заменяем черный пиксель на среднее значение\n",
        "      image[i, j] = mean_pixel\n",
        "\n",
        "\n",
        "  # получаем координаты белых пикселей\n",
        "  white_pixels = np.where(np.all(image == [255, 255, 255], axis=-1))\n",
        "  # print(white_pixels)\n",
        "\n",
        "  # заменяем черные пиксели на среднее значение пикселей в окрестности 3x3\n",
        "  for i, j in zip(white_pixels[0], white_pixels[1]):\n",
        "      # получаем окрестность 3x3 пикселя\n",
        "      neighborhood = image[max(0, i-1):min(i+2, image.shape[0]), max(0, j-1):min(j+2, image.shape[1])]\n",
        "      # отсеиваем соль и перец чтобы не мешались\n",
        "      neighborhood1 = neighborhood.reshape((-1, 3))\n",
        "      neighborhood1 = neighborhood1[np.nonzero(np.sum(neighborhood1, axis=1))]\n",
        "      neighborhood1 = neighborhood1[np.nonzero(np.sum(neighborhood1, axis=1) != 255*3)]\n",
        "      if not neighborhood1.any():  # на случай если пиксель полностью окружён шумом\n",
        "          neighborhood1 = neighborhood\n",
        "      # считаем среднее значение пикселей\n",
        "      mean_pixel = np.mean(neighborhood, axis=(0, 1))\n",
        "      # заменяем белый пиксель на среднее значение\n",
        "      image[i, j] = mean_pixel"
      ],
      "metadata": {
        "id": "SI4XAzuqegoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in train_data['images']:\n",
        "  black_white_point(image)\n",
        "\n",
        "for image in test_data['images']:\n",
        "  black_white_point(image)"
      ],
      "metadata": {
        "id": "46wveB5TenGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## АУГМ"
      ],
      "metadata": {
        "id": "pguT6IV8e272"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "TPGgDZQ7fm7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  # layers.RandomFlip(\"horizontal\"),\n",
        "  layers.RandomRotation(0.1),\n",
        "  layers.RandomContrast(0.4),\n",
        "  layers.RandomBrightness(0.2),\n",
        "  layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "68msKT4_enkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orig_ds = tf.data.Dataset.from_tensor_slices((train_data['images'], train_data['labels']))\n",
        "print(len(orig_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI8ja5DXh_WK",
        "outputId": "50ee5636-698f-42cb-f3f4-08495accc32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augment_ds = orig_ds\n",
        "# ко всем цифрам 0, 8, 9 будут прибавлены по две аугментированной\n",
        "dict_augment_for_label = {\n",
        "    0: 2,\n",
        "    8: 2,\n",
        "    9: 2\n",
        "}\n",
        "for label, value in dict_augment_for_label.items():\n",
        "  for i in range(value):\n",
        "    certain_label = orig_ds.filter(lambda x, y: y == label)\n",
        "    aug_ds = certain_label.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y))\n",
        "    augment_ds = augment_ds.concatenate(aug_ds)"
      ],
      "metadata": {
        "id": "enTv3y75ipcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_augment_for_label = {\n",
        "    7: 2,\n",
        "    6: 2,\n",
        "}\n",
        "# к 75% цифр 7,6 будут прибавлены аугментированные\n",
        "percent_augment_label = 0.75\n",
        "for label, value in dict_augment_for_label.items():\n",
        "  for i in range(value):\n",
        "    certain_label = orig_ds.filter(lambda x, y: y == label).shuffle(1000)\n",
        "    certain_label = certain_label.take(int(len([i for i in certain_label]) * percent_augment_label))\n",
        "    aug_ds = certain_label.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y))\n",
        "    augment_ds = augment_ds.concatenate(aug_ds)\n"
      ],
      "metadata": {
        "id": "Z3VJ6_zgjy8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ко всем цифрам 4, 5 будут прибавлены по одной аугментированной\n",
        "dict_augment_for_label = {\n",
        "    4: 1,\n",
        "    5: 1,\n",
        "}\n",
        "for label, value in dict_augment_for_label.items():\n",
        "  for i in range(value):\n",
        "    certain_label = orig_ds.filter(lambda x, y: y == label)\n",
        "    aug_ds = certain_label.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y))\n",
        "    augment_ds = augment_ds.concatenate(aug_ds)"
      ],
      "metadata": {
        "id": "0OFg7GvylCr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_augment_for_label = {\n",
        "    3: 1,\n",
        "}\n",
        "# к 70% цифр 3 будут прибавлены аугментированные\n",
        "percent_augment_label = 0.7\n",
        "for label, value in dict_augment_for_label.items():\n",
        "  for i in range(value):\n",
        "    certain_label = orig_ds.filter(lambda x, y: y == label).shuffle(1000)\n",
        "    certain_label = certain_label.take(int(len([i for i in certain_label]) * percent_augment_label))\n",
        "    aug_ds = certain_label.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y))\n",
        "    augment_ds = augment_ds.concatenate(aug_ds)\n"
      ],
      "metadata": {
        "id": "MMq1DKCDlZtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_augment_for_label = {\n",
        "    2: 1,\n",
        "}\n",
        "# к 27% цифр 2 будут прибавлены аугментированные\n",
        "percent_augment_label = 0.27\n",
        "for label, value in dict_augment_for_label.items():\n",
        "  for i in range(value):\n",
        "    certain_label = orig_ds.filter(lambda x, y: y == label).shuffle(1000)\n",
        "    certain_label = certain_label.take(int(len([i for i in certain_label]) * percent_augment_label))\n",
        "    aug_ds = certain_label.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y))\n",
        "    augment_ds = augment_ds.concatenate(aug_ds)\n"
      ],
      "metadata": {
        "id": "Sjote0-ul0KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_orig_ds = len([i for i in augment_ds])"
      ],
      "metadata": {
        "id": "UkopfzKpiXsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augment_ds"
      ],
      "metadata": {
        "id": "Qe4CCZlBqGWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orig_ds = augment_ds\n",
        "orig_val_ds, orig_train_ds = orig_ds.take(int(len_orig_ds* 0.05)) , orig_ds.skip(int(len_orig_ds * 0.05))\n",
        "# print(len(orig_val_ds), len(orig_train_ds))\n",
        "print(len([i for i in orig_val_ds]))"
      ],
      "metadata": {
        "id": "pyAA0yqej72j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "IMG_SIZE = 32\n",
        "\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "def prepare(ds, shuffle=False, augment=False):\n",
        "  # Resize and rescale all datasets.\n",
        "  ds = ds.map(lambda x, y: (resize_and_rescale(x), y), \n",
        "              num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(1000)\n",
        "\n",
        "  # Batch all datasets.\n",
        "  ds = ds.batch(batch_size).map(lambda x, y: (x, tf.one_hot(y, depth=10)))\n",
        "\n",
        "  # Use data augmentation only on the training set.\n",
        "  if augment:\n",
        "    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n",
        "                num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  # Use buffered prefetching on all datasets.\n",
        "  return ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "train_ds = prepare(orig_train_ds, shuffle=True)\n",
        "val_ds = prepare(orig_val_ds)\n",
        "# test_ds = prepare(test_ds)"
      ],
      "metadata": {
        "id": "_pvQQRTlj7zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "#СОЗДАЕМ МОДЕЛЬ \n",
        "input_shape = train_data['images'].shape[-3:]\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, BatchNormalization, Activation, Add,Dropout\n",
        "\n",
        "\n",
        "# Определение функции для создания Residual блоков\n",
        "def residual_block(inputs, num_filters, kernel_size, strides):\n",
        "    x = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Add()([x, inputs])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Создание модели ResNet-50 архитектуры\n",
        "inputs = Input(shape=input_shape)\n",
        "x = Conv2D(64, kernel_size=3, strides=1, padding='same')(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = residual_block(x, 64, 3, 1)\n",
        "x = residual_block(x, 64, 3, 1)\n",
        "x = residual_block(x, 64, 3, 1)\n",
        "x = Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = residual_block(x, 128, 3, 1)\n",
        "x = residual_block(x, 128, 3, 1)\n",
        "x = residual_block(x, 128, 3, 1)\n",
        "x = Conv2D(256, kernel_size=3, strides=2, padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = residual_block(x, 256, 3, 1)\n",
        "x = residual_block(x, 256, 3, 1)\n",
        "x = residual_block(x, 256, 3, 1)\n",
        "x = Flatten()(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# Компиляция и обучение модели\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "zc0PF9nBj7v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install livelossplot"
      ],
      "metadata": {
        "id": "dVSw9nRarVl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from livelossplot import PlotLossesKeras\n",
        "import time \n",
        "\n",
        "start = time.time() \n",
        "model_history = model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=[PlotLossesKeras()], verbose=False)\n",
        "end = time.time() - start \n",
        "print(\"Время: \"+ str(end) +' секунд')"
      ],
      "metadata": {
        "id": "zrWQ-yIOrZRb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}